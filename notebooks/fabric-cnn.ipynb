{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9014918,"sourceType":"datasetVersion","datasetId":5408843},{"sourceId":9080830,"sourceType":"datasetVersion","datasetId":5478601}],"dockerImageVersionId":30747,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nfrom torch import nn\nimport torch.nn.functional as F\n\nclass ImageClassificationBase(nn.Module):\n    def training_step(self, batch):\n        images, labels = batch\n        out = self(images)  # Generate predictions\n        loss = F.cross_entropy(out, labels)  # Calculate loss\n        return loss\n\n    def validation_step(self, batch):\n        images, labels = batch\n        out = self(images)  # Generate predictions\n        loss = F.cross_entropy(out, labels)  # Calculate loss\n        acc = accuracy(out, labels)  # Calculate accuracy\n        return {'val_loss': loss.detach(), 'val_acc': acc}\n\n    def validation_epoch_end(self, outputs):\n        batch_losses = [x['val_loss'] for x in outputs]\n        epoch_loss = torch.stack(batch_losses).mean()  # Combine losses\n        batch_accs = [x['val_acc'] for x in outputs]\n        epoch_acc = torch.stack(batch_accs).mean()  # Combine accuracies\n        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}\n\n    def epoch_end(self, epoch, result):\n        print(\"Epoch [{}], train_loss: {:.4f}, val_loss: {:.4f}, val_acc: {:.4f}\".format(\n            epoch, result['train_loss'], result['val_loss'], result['val_acc']))\n\n\ndef accuracy(outputs, labels):\n    _, preds = torch.max(outputs, dim=1)\n    return torch.tensor(torch.sum(preds == labels).item() / len(preds))\n\n\nclass Cifar10CnnModel(ImageClassificationBase):\n    def __init__(self):\n        super().__init__()\n        self.network = nn.Sequential(\n            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(2, 2),  # output: 64 x 16 x 16\n\n            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(2, 2),  # output: 128 x 8 x 8\n\n            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(2, 2),  # output: 256 x 4 x 4\n\n            nn.Flatten(),\n            nn.Linear(576, 1024),\n            nn.ReLU(),\n            nn.Linear(1024, 512),\n            nn.ReLU(),\n            nn.Linear(512, 13))\n\n    def forward(self, xb):\n        return self.network(xb)\n\n\ndef get_default_device():\n    \"\"\"Pick GPU if available, else CPU\"\"\"\n    if torch.cuda.is_available():\n        return torch.device('cuda')\n    else:\n        return torch.device('cpu')\n\n\ndef to_device(data, device):\n    \"\"\"Move tensor(s) to chosen device\"\"\"\n    if isinstance(data, (list, tuple)):\n        return [to_device(x, device) for x in data]\n    return data.to(device, non_blocking=True)\n\n\nclass DeviceDataLoader():\n    \"\"\"Wrap a dataloader to move data to a device\"\"\"\n\n    def __init__(self, dl, device):\n        self.dl = dl\n        self.device = device\n\n    def __iter__(self):\n        \"\"\"Yield a batch of data after moving it to device\"\"\"\n        for b in self.dl:\n            yield to_device(b, self.device)\n\n    def __len__(self):\n        \"\"\"Number of batches\"\"\"\n        return len(self.dl)\n\n\nclass SimpleCNN(nn.Module):\n    def __init__(self):\n        super(SimpleCNN, self).__init__()\n        # First convolutional layer\n        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, stride=1, padding=1)\n        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n\n        # Second convolutional layer\n        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)\n\n        # Third convolutional layer\n        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1)\n\n        # Fully connected layer\n        self.fc1 = nn.Linear(128 * 25 * 25, 256)\n        self.fc2 = nn.Linear(256, 128)\n        self.fc3 = nn.Linear(128, 13)  # 13 output classes\n\n    def forward(self, x):\n        x = self.pool(F.relu(self.conv1(x)))  # Apply conv1 + relu + max pooling\n        x = self.pool(F.relu(self.conv2(x)))  # Apply conv2 + relu + max pooling\n        x = self.pool(F.relu(self.conv3(x)))  # Apply conv3 + relu + max pooling\n\n        x = x.view(-1, 128 * 25 * 25)  # Flatten the output from conv layers\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = self.fc3(x)  # Output layer\n\n        return x\n\n\nimport torch.nn.functional as F\n\nclass GeneralizedCNN(nn.Module):\n    def __init__(self, input_size, num_classes):\n        super(GeneralizedCNN, self).__init__()\n        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1)\n        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n        self.dropout = nn.Dropout(0.5)\n        fc_input_size = (input_size[0] // 8) * (input_size[1] // 8) * 128\n        self.fc1 = nn.Linear(fc_input_size, 512)\n        self.fc2 = nn.Linear(512, num_classes)\n        \n    def forward(self, x):\n        x = self.pool(F.relu(self.conv1(x)))\n        x = self.pool(F.relu(self.conv2(x)))\n        x = self.pool(F.relu(self.conv3(x)))\n        x = x.view(-1, self.num_flat_features(x))\n        x = F.relu(self.fc1(x))\n        x = self.dropout(x)\n        x = self.fc2(x)\n        return x\n    \n    def num_flat_features(self, x):\n        size = x.size()[1:]  # All dimensions except the batch dimension\n        num_features = 1\n        for s in size:\n            num_features *= s\n        return num_features\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-08-04T16:16:41.923234Z","iopub.execute_input":"2024-08-04T16:16:41.924115Z","iopub.status.idle":"2024-08-04T16:16:43.669302Z","shell.execute_reply.started":"2024-08-04T16:16:41.924081Z","shell.execute_reply":"2024-08-04T16:16:43.668502Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom torch import nn, optim\nimport matplotlib.pyplot as plt\n\nfrom torch.utils.data import random_split\nfrom torchvision.datasets import ImageFolder\nfrom torchvision.transforms import ToTensor\nfrom torchvision import transforms\nfrom torch.utils.data.dataloader import DataLoader\nimport os\n\ndata_dir = '/kaggle/input/fabric-romo/romo'\n\n# data_dir = \"/kaggle/input/pattern-recognition/pattern-recognition\"\n\ntransform = transforms.Compose([\n    transforms.Resize((32, 32)),  # Resize the image to 150x150\n    transforms.RandomHorizontalFlip(),\n    transforms.ToTensor(),          # Convert the image to a tensor\n#     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalize the image\n    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n\n])\ndataset = ImageFolder(data_dir+'/train', transform=transform)\nimg, label = dataset[0]\nprint(img.size, label)\nprint(dataset.classes)\n\n\nprint(len(dataset))\n\ntotal_size = len(dataset)\nval_size = int(0.3 * total_size)\ntrain_size = len(dataset) - val_size\n\ntrain_ds, val_ds = random_split(dataset, [train_size, val_size])\nlen(train_ds), len(val_ds)\n\nbatch_size = 16\ntrain_dl = DataLoader(train_ds, batch_size, shuffle=True, num_workers=4, pin_memory=True)\nval_dl = DataLoader(val_ds, batch_size, num_workers=4, pin_memory=True)\n\n# model = GeneralizedCNN(input_size=(150, 150), num_classes=18)\nmodel = GeneralizedCNN(input_size=(32, 32), num_classes=18)\n\ndevice = get_default_device()\n\ntrain_dl = DeviceDataLoader(train_dl, device)\nval_dl = DeviceDataLoader(val_dl, device)\nto_device(model, device)\n\n\n# Initialize the model, loss function, and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\n# Training loop\n\n# Initialize lists to store loss and accuracy\ntrain_losses = []\nval_losses = []\ntrain_accuracies = []\nval_accuracies = []\n\nnum_epochs = 100\nfor epoch in range(num_epochs):\n    # Training phase\n    model.train()\n    running_loss = 0.0\n    running_corrects = 0\n    for inputs, labels in train_dl:\n        inputs, labels = inputs.to(device), labels.to(device)\n        optimizer.zero_grad()\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        \n        running_loss += loss.item() * inputs.size(0)\n        _, preds = torch.max(outputs, 1)\n        running_corrects += torch.sum(preds == labels.data)\n    \n    epoch_loss = running_loss / train_size\n    epoch_acc = running_corrects.double() / train_size\n    train_losses.append(epoch_loss)\n    train_accuracies.append(epoch_acc)\n    \n    # Validation phase\n    model.eval()\n    val_running_loss = 0.0\n    val_running_corrects = 0\n    with torch.no_grad():\n        for inputs, labels in val_dl:\n            inputs, labels = inputs.to(device), labels.to(device)\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            val_running_loss += loss.item() * inputs.size(0)\n            _, preds = torch.max(outputs, 1)\n            val_running_corrects += torch.sum(preds == labels.data)\n    \n    val_loss = val_running_loss / val_size\n    val_acc = val_running_corrects.double() / val_size\n    val_losses.append(val_loss)\n    val_accuracies.append(val_acc)\n    \n    print(f'Epoch {epoch}/{num_epochs - 1}, '\n          f'Train Loss: {epoch_loss:.4f}, Train Accuracy: {epoch_acc:.4f}, '\n          f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_acc:.4f}')\nprint('Finished Training')\n\n\n# Plot loss and accuracy\nepochs = range(num_epochs)\n\nplt.figure(figsize=(12, 4))\nplt.subplot(1, 2, 1)\nplt.plot(epochs, train_losses, label='Training Loss')\nplt.plot(epochs, val_losses, label='Validation Loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.title('Training and Validation Loss')\n\nplt.subplot(1, 2, 2)\nplt.plot(epochs, train_accuracies, label='Training Accuracy')\nplt.plot(epochs, val_accuracies, label='Validation Accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.title('Training and Validation Accuracy')\n\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-08-04T16:16:43.671225Z","iopub.execute_input":"2024-08-04T16:16:43.671857Z","iopub.status.idle":"2024-08-04T16:18:17.650575Z","shell.execute_reply.started":"2024-08-04T16:16:43.671823Z","shell.execute_reply":"2024-08-04T16:18:17.648968Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"<built-in method size of Tensor object at 0x79117ba2b740> 0\n['abstract', 'animals', 'damask', 'dot_spot', 'floral', 'geometric', 'herringbone_chevron', 'ikat', 'moire', 'motif', 'ombre', 'paisley', 'plaid_check', 'plain_solid', 'semi_plain', 'small_scale', 'stripe', 'trellis']\n9630\nEpoch 0/99, Train Loss: 2.3850, Train Accuracy: 0.2074, Val Loss: 2.3313, Val Accuracy: 0.2118\nEpoch 1/99, Train Loss: 2.2776, Train Accuracy: 0.2308, Val Loss: 2.2094, Val Accuracy: 0.2440\nEpoch 2/99, Train Loss: 2.2217, Train Accuracy: 0.2402, Val Loss: 2.2771, Val Accuracy: 0.2260\nEpoch 3/99, Train Loss: 2.1573, Train Accuracy: 0.2743, Val Loss: 2.0739, Val Accuracy: 0.3115\nEpoch 4/99, Train Loss: 2.0948, Train Accuracy: 0.2980, Val Loss: 2.1019, Val Accuracy: 0.2831\nEpoch 5/99, Train Loss: 2.0557, Train Accuracy: 0.3123, Val Loss: 2.0408, Val Accuracy: 0.3250\nEpoch 6/99, Train Loss: 2.0342, Train Accuracy: 0.3201, Val Loss: 2.0191, Val Accuracy: 0.3423\nEpoch 7/99, Train Loss: 1.9939, Train Accuracy: 0.3357, Val Loss: 2.0276, Val Accuracy: 0.3399\nEpoch 8/99, Train Loss: 1.9780, Train Accuracy: 0.3402, Val Loss: 1.9738, Val Accuracy: 0.3638\nEpoch 9/99, Train Loss: 1.9555, Train Accuracy: 0.3458, Val Loss: 1.9206, Val Accuracy: 0.3766\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[2], line 71\u001b[0m\n\u001b[1;32m     69\u001b[0m running_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m     70\u001b[0m running_corrects \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 71\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m inputs, labels \u001b[38;5;129;01min\u001b[39;00m train_dl:\n\u001b[1;32m     72\u001b[0m     inputs, labels \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     73\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n","Cell \u001b[0;32mIn[1], line 93\u001b[0m, in \u001b[0;36mDeviceDataLoader.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__iter__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m     92\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Yield a batch of data after moving it to device\"\"\"\u001b[39;00m\n\u001b[0;32m---> 93\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m b \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdl:\n\u001b[1;32m     94\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m to_device(b, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1328\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1325\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data)\n\u001b[1;32m   1327\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 1328\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1329\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1330\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable:\n\u001b[1;32m   1331\u001b[0m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1284\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1282\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m   1283\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_thread\u001b[38;5;241m.\u001b[39mis_alive():\n\u001b[0;32m-> 1284\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1285\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[1;32m   1286\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1132\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1119\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_try_get_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m_utils\u001b[38;5;241m.\u001b[39mMP_STATUS_CHECK_INTERVAL):\n\u001b[1;32m   1120\u001b[0m     \u001b[38;5;66;03m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[1;32m   1121\u001b[0m     \u001b[38;5;66;03m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1129\u001b[0m     \u001b[38;5;66;03m# Returns a 2-tuple:\u001b[39;00m\n\u001b[1;32m   1130\u001b[0m     \u001b[38;5;66;03m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[1;32m   1131\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1132\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1133\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n\u001b[1;32m   1134\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1135\u001b[0m         \u001b[38;5;66;03m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[1;32m   1136\u001b[0m         \u001b[38;5;66;03m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[1;32m   1137\u001b[0m         \u001b[38;5;66;03m# worker failures.\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/queue.py:180\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m remaining \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m:\n\u001b[1;32m    179\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[0;32m--> 180\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnot_empty\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mremaining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    181\u001b[0m item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get()\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnot_full\u001b[38;5;241m.\u001b[39mnotify()\n","File \u001b[0;32m/opt/conda/lib/python3.10/threading.py:324\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    323\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 324\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    325\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    326\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m waiter\u001b[38;5;241m.\u001b[39macquire(\u001b[38;5;28;01mFalse\u001b[39;00m)\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"# save model\ntorch.save(model, \"/kaggle/working/pr.pth\")","metadata":{"execution":{"iopub.status.busy":"2024-08-04T16:18:17.651397Z","iopub.status.idle":"2024-08-04T16:18:17.651750Z","shell.execute_reply.started":"2024-08-04T16:18:17.651583Z","shell.execute_reply":"2024-08-04T16:18:17.651597Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dt = ImageFolder(data_dir+'/train', transform=transform)\ndt.classes","metadata":{"execution":{"iopub.status.busy":"2024-08-04T16:18:17.653447Z","iopub.status.idle":"2024-08-04T16:18:17.653834Z","shell.execute_reply.started":"2024-08-04T16:18:17.653613Z","shell.execute_reply":"2024-08-04T16:18:17.653626Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom torchvision import datasets, transforms\nfrom torch.utils.data import DataLoader\n\n# Define transformations for the test data\ntransform = transforms.Compose([\n    transforms.Resize((150, 150)),  # Resize images to 150x150\n    transforms.ToTensor(),          # Convert images to tensors\n    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalize images\n])\n\n# Load your test dataset (example using CIFAR-10, replace with your dataset)\ntest_dataset = ImageFolder(data_dir+'/test', transform=transform)\n# Define the test data loader\ntest_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=2)\n\n# Define the class names (example using CIFAR-10 class names)\nclass_names = dt.classes\n\n# Function to calculate the accuracy\ndef calculate_accuracy(model, data_loader, class_names, device):\n    model.eval()  # Set the model to evaluation mode\n    class_correct = [0] * len(class_names)\n    class_total = [0] * len(class_names)\n    total_correct = 0\n    total_samples = 0\n    \n    with torch.no_grad():\n        for inputs, labels in data_loader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            outputs = model(inputs)\n            _, predicted = torch.max(outputs, 1)\n            c = (predicted == labels).squeeze()\n            for i in range(len(labels)):\n                label = labels[i]\n                class_correct[label] += c[i].item()\n                class_total[label] += 1\n            total_correct += (predicted == labels).sum().item()\n            total_samples += labels.size(0)\n    \n    overall_accuracy = 100 * total_correct / total_samples\n    print(f'Overall Accuracy: {overall_accuracy:.2f}%')\n    \n    for i in range(len(class_names)):\n        if class_total[i] > 0:\n            accuracy = 100 * class_correct[i] / class_total[i]\n            print(f'Accuracy of {class_names[i]}: {accuracy:.2f}%')\n        else:\n            print(f'Accuracy of {class_names[i]}: N/A (no samples)')\n    \n    return overall_accuracy, class_correct, class_total\n\n# Evaluate the model on the test set\ncalculate_accuracy(model, test_loader, class_names, device)","metadata":{"execution":{"iopub.status.busy":"2024-08-04T16:18:17.655054Z","iopub.status.idle":"2024-08-04T16:18:17.655410Z","shell.execute_reply.started":"2024-08-04T16:18:17.655253Z","shell.execute_reply":"2024-08-04T16:18:17.655266Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom torchvision import transforms\nfrom PIL import Image\n\n# Define the transformations for the image\ntransform = transforms.Compose([\n    transforms.Resize((150, 150)),  # Resize the image to 150x150\n    transforms.RandomHorizontalFlip(),\n    transforms.ToTensor(),          # Convert the image to a tensor\n#     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalize the image\n    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n\n])\n\n# Detect the available device (CPU or GPU)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Load the trained model (assuming it's saved as 'model.pth')\ninput_size = (150, 150)\nnum_classes = 18\nmodel = GeneralizedCNN(input_size=input_size, num_classes=num_classes)\nmodel = torch.load('/kaggle/working/pr.pth'))\nmodel.to(device)  # Move the model to the detected device\nmodel.eval()  # Set the model to evaluation mode\n\n# Function to predict the class of an unknown image\ndef predict_image(image_path, model, transform, class_names, device):\n    # Load the image\n    image = Image.open(image_path).convert('RGB')\n    \n    # Preprocess the image\n    image = transform(image).unsqueeze(0)  # Add batch dimension\n    image = image.to(device)  # Move the image to the detected device\n    \n    # Make prediction\n    with torch.no_grad():\n        outputs = model(image)\n        _, predicted = torch.max(outputs, 1)\n    \n    # Get the predicted class name\n    predicted_class = class_names[predicted.item()]\n    return predicted_class\n\n# Example usage\nimage_path = '/kaggle/input/fabric-romo/romo/test/other/7782-01-odette-chamois_01.jpg'  # Replace with the path to your image\nclass_names = dt.classes  # Replace with your class names if different\n\npredicted_class = predict_image(image_path, model, transform, class_names, device)\nprint(f'The predicted class for the image is: {predicted_class}')\n","metadata":{"execution":{"iopub.status.busy":"2024-08-04T16:18:17.656685Z","iopub.status.idle":"2024-08-04T16:18:17.657046Z","shell.execute_reply.started":"2024-08-04T16:18:17.656850Z","shell.execute_reply":"2024-08-04T16:18:17.656863Z"},"trusted":true},"execution_count":null,"outputs":[]}]}